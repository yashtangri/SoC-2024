{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    \n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T16:54:07.899341Z","iopub.execute_input":"2024-06-09T16:54:07.900316Z","iopub.status.idle":"2024-06-09T16:54:07.906354Z","shell.execute_reply.started":"2024-06-09T16:54:07.900278Z","shell.execute_reply":"2024-06-09T16:54:07.905239Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM \n\ntokenizer1 = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\nmodel1 = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:53:28.671642Z","iopub.execute_input":"2024-06-09T17:53:28.672082Z","iopub.status.idle":"2024-06-09T17:53:38.919520Z","shell.execute_reply.started":"2024-06-09T17:53:28.672048Z","shell.execute_reply":"2024-06-09T17:53:38.918424Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"prompt = \"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\"\nencoding = tokenizer1(prompt, return_tensors='pt')\n\n# Extract input_ids and attention_mask\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\n#input_ids = tokenizer1.encode(prompt, return_tensors='pt')\noutput = model1.generate(\n    input_ids, max_length = 200, num_return_sequences=1,\n    attention_mask=attention_mask,\n    #temperature=0.7,  \n    #top_k=50,        \n   # top_p=0.9,       \n    do_sample=True\n)\ngenerated_text = tokenizer1.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:12:21.853350Z","iopub.execute_input":"2024-06-09T17:12:21.854375Z","iopub.status.idle":"2024-06-09T17:12:28.617980Z","shell.execute_reply.started":"2024-06-09T17:12:21.854335Z","shell.execute_reply":"2024-06-09T17:12:28.616917Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day. You can also post that in your social media.\n\nFor example, on Monday, February 29, you said, \"World Environment Day is a great occasion for journalists. Let's tweet about it and you will grow up to be a great reporter.\"\n\nThere is a massive media and social phenomenon that is thriving at @WorldEnvironmentDay. Now some people see that as an excuse to not publish. People think it is the beginning of a career. It is. When it is, people will tell you to don't publish because not only is publishing more effective, it is a career move.\n\nPeople also forget how important it is to keep people active, so that when they start to interact with you, they feel less intimidated, and they take time to learn. In fact, if you are not taking time to learn, they will never get as much out\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer2 = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\nmodel2 = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:40:59.380043Z","iopub.execute_input":"2024-06-09T17:40:59.381238Z","iopub.status.idle":"2024-06-09T17:42:02.982643Z","shell.execute_reply.started":"2024-06-09T17:40:59.381197Z","shell.execute_reply":"2024-06-09T17:42:02.981610Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2798fe9d47224fa982bd4b4423702e02"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\"\nencoding = tokenizer2(prompt, return_tensors='pt')\n\n# Extract input_ids and attention_mask\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\n#input_ids = tokenizer2.encode(prompt, return_tensors='pt')\noutput = model2.generate(\n    input_ids, max_length = 200, num_return_sequences=1,\n    attention_mask=attention_mask,\n    #temperature=0.7,  \n    #top_k=50,        \n   # top_p=0.9,       \n    do_sample=True\n)\ngenerated_text = tokenizer2.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:42:15.996098Z","iopub.execute_input":"2024-06-09T17:42:15.996505Z","iopub.status.idle":"2024-06-09T17:43:59.258365Z","shell.execute_reply.started":"2024-06-09T17:42:15.996472Z","shell.execute_reply":"2024-06-09T17:43:59.257357Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\n\nPrompt: Write a tweet that highlights the importance of protecting the environment and encourages people to take action on World Environment Day.\n\nExample: \"Join us on World Environment Day to make a difference! Let's work together to protect our planet and create a sustainable future for all. #WorldEnvironmentDay #Sustainability #EcoFriendly\"\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer3 = AutoTokenizer.from_pretrained(\"microsoft/wavecoder-ultra-6.7b\")\nmodel3 = AutoModelForCausalLM.from_pretrained(\"microsoft/wavecoder-ultra-6.7b\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:53:38.921375Z","iopub.execute_input":"2024-06-09T17:53:38.921843Z","iopub.status.idle":"2024-06-09T17:55:35.008958Z","shell.execute_reply.started":"2024-06-09T17:53:38.921811Z","shell.execute_reply":"2024-06-09T17:55:35.007263Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fb3e152eda431495f6640b60f75ebf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\"\nencoding = tokenizer3(prompt, return_tensors='pt')\n\n# Extract input_ids and attention_mask\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\n#input_ids = tokenizer3.encode(prompt, return_tensors='pt')\noutput = model3.generate(\n    input_ids, max_length = 200, num_return_sequences=1,\n    attention_mask=attention_mask,\n    #temperature=0.7,  \n    #top_k=50,        \n   # top_p=0.9,       \n    do_sample=True\n)\ngenerated_text = tokenizer3.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T17:58:03.260067Z","iopub.execute_input":"2024-06-09T17:58:03.260519Z","iopub.status.idle":"2024-06-09T17:58:09.433193Z","shell.execute_reply.started":"2024-06-09T17:58:03.260484Z","shell.execute_reply":"2024-06-09T17:58:09.432049Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:32014 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM \ntokenizer4 = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\nmodel4 = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:01:59.435204Z","iopub.execute_input":"2024-06-09T18:01:59.435638Z","iopub.status.idle":"2024-06-09T18:02:39.229364Z","shell.execute_reply.started":"2024-06-09T18:01:59.435602Z","shell.execute_reply":"2024-06-09T18:02:39.228290Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdb9443ba196491087890dc2fe15b222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711fa36208bc4df882bc593c849f8e3c"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = \"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\"\nencoding = tokenizer4(prompt, return_tensors='pt')\n\n# Extract input_ids and attention_mask\ninput_ids = encoding['input_ids']\nattention_mask = encoding['attention_mask']\n#input_ids = tokenizer4.encode(prompt, return_tensors='pt')\noutput = model4.generate(\n    input_ids, max_length = 200, num_return_sequences=1,\n    attention_mask=attention_mask,\n    #temperature=0.7,  \n    #top_k=50,        \n   # top_p=0.9,       \n    do_sample=True\n)\ngenerated_text = tokenizer4.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:04:02.202193Z","iopub.execute_input":"2024-06-09T18:04:02.203335Z","iopub.status.idle":"2024-06-09T18:04:06.238888Z","shell.execute_reply.started":"2024-06-09T18:04:02.203296Z","shell.execute_reply":"2024-06-09T18:04:06.237562Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"You are an intelligent assistant dedicated to creating engaging tweets to gain public attention. Create a tweet to promote World Environment Day.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}